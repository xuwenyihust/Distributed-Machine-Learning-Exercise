{"cells":[{"cell_type":"markdown","source":["# Click-Through Rate Prediction\n### Notebook created by [Wenyi Xu](https://github.com/xuwenyihust)\n### Create a [click-through rate](https://www.kaggle.com/c/criteo-display-ad-challenge) (CTR) prediction pipeline."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nfrom pyspark.mllib.linalg import SparseVector"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### 1. Parse CTR Data\n\nView Criteo's agreement."],"metadata":{}},{"cell_type":"code","source":["from IPython.lib.display import IFrame\n\nIFrame(\"http://labs.criteo.com/downloads/2014-kaggle-display-advertising-challenge-dataset/\", 600, 350)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Load the data"],"metadata":{}},{"cell_type":"code","source":["# Hidden path\nrawData = sc.textFile('/FileStore/tables/86sw321e1469469485636/dac_sample.txt').map(lambda x: x.replace('\\t', ','))\nprint rawData.take(1)\nprint type(rawData)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#### Split the dataset into training, validation and test sets\n\nSpecify the weights & seed for randomSplit method.\n\n**Training : Validation : Test** will be **8 : 1 : 1**."],"metadata":{}},{"cell_type":"code","source":["weights = [.8, .1, .1]\nseed = 42"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["rawTrainData, rawValidationData, rawTestData = rawData.randomSplit(weights, seed)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["**Cache** the splitted datasets, since we will be repeatedly using them."],"metadata":{}},{"cell_type":"code","source":["rawTrainData.cache()\nrawValidationData.cache()\nrawTestData.cache()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["**Data preview**."],"metadata":{}},{"cell_type":"code","source":["nTrain = rawTrainData.count()\nnVal = rawValidationData.count()\nnTest = rawTestData.count()\nprint nTrain, nVal, nTest, nTrain + nVal + nTest\nprint rawData.take(1)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["#### Extract features\n\nSplit each datapoint of type string into different field.\n\nDrop the first field -> **label** (clicked or not)\n\nSave the remaining fields -> **features**"],"metadata":{}},{"cell_type":"markdown","source":["Define a *parse_data_point* function, input each data point(row), return a list of **(featureID, value)** tuples."],"metadata":{}},{"cell_type":"code","source":["def parsePoint(point):\n    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n\n    Note:\n        featureIDs should start at 0 and increase to the number of features - 1.\n\n    Args:\n        point (str): A comma separated string where the first value is the label and the rest\n            are features.\n\n    Returns:\n        list: A list of (featureID, value) tuples.\n    \"\"\"\n    features = point.split(',')[1:]\n    return [(idx, value) for (idx, value) in enumerate(features)]"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["parsedTrainFeat = rawTrainData.map(parsePoint)\nprint parsedTrainFeat.take(1)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["We can see that now the string of features has been splitted into a list of features.\n\nCount the **number of distinct values for each feature**."],"metadata":{}},{"cell_type":"code","source":["numCategories = (parsedTrainFeat\n                 # Flatten all the elements in the list\n                 .flatMap(lambda x: x)\n                 # Drop the duplicated values for features\n                 .distinct()\n                 # Set feature value to 1 for the convenience of counting\n                 .map(lambda x: (x[0], 1))\n                 # Count how many times each key (featureID) occurs\n                 .reduceByKey(lambda x, y: x + y)\n                 .sortByKey()\n                 .collect())\n\nprint numCategories[2][1]"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### 2. Generate OHE Features"],"metadata":{}},{"cell_type":"markdown","source":["#### Construct an OHE dictionary\n\nCategorical feature:  **(featureID, category)** tuple.\n\nOHE dictionary:  **Map** each tuple **to** a distinct **integer**.\n\nFunction:\n\n- Input the lists of (featureID, category) tuples.\n- Flatten the lists to get all the unique (featureID, category) tuples.\n- Attach a unique integer to each distinct feature to create the dictionary."],"metadata":{}},{"cell_type":"code","source":["def createOneHotDict(inputData):\n    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n\n    Args:\n        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n            made up of a list of (featureID, value) tuples.\n\n    Returns:\n        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n            unique integers.\n    \"\"\"\n    distinctFeats = (inputData\n                       .flatMap(lambda row: row)\n                       .distinct())\n    \n    return (distinctFeats\n                           # Zips this RDD with its element indices\n                           # (featureID, value) -> ((featureID, value), int)\n                           .zipWithIndex()\n                           # Return the key-value pairs in this RDD to the master as a dictionary.\n                           .collectAsMap())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Create a **OHE dictionary** based on the **parsedTrainFeat**:\n\n    [[(0, u'1'), (1, u'1'), (2, u'5'), (3, u'0'), (4, u'1382'), (5, u'4'), (6, u'15'), ..."],"metadata":{}},{"cell_type":"code","source":["ctrOHEDict = createOneHotDict(parsedTrainFeat)\nnumCtrOHEFeats = len(ctrOHEDict.keys())\nprint 'Number of elements in the dictionary: ', numCtrOHEFeats"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["print \"(0, '1'): \", ctrOHEDict[(0, '1')]\nprint \"(0, '3'): \", ctrOHEDict[(0, '3')]\nprint \"(1, '4'): \", ctrOHEDict[(1, '4')]\nprint \"(3, '1'): \", ctrOHEDict[(3, '1')]\nprint \"(9, '3'): \", ctrOHEDict[(9, '3')]"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#### Define a OHE function\n\nUse it to generate **OHE features** from the original categorical data.\n\nThe OHE features should be **SparseVector** format to reduce the storage & computational burdens."],"metadata":{}},{"cell_type":"code","source":["def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n\n    Note:\n        You should ensure that the indices used to create a SparseVector are sorted.\n\n    Args:\n        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n            value).\n\n    Returns:\n        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n            identifiers for the (featureID, value) combinations that occur in the observation and\n            with values equal to 1.0.\n    \"\"\"\n    return SparseVector(numOHEFeats, [(OHEDict[(featID, value)],1) for (featID, value) in rawFeats])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### Apply OHE to the dataset\n\nFor each data point (sample):\n\n**data point -> one-hot encoded (categorical to numerical) -> sparse vector -> labeled point**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["def parseOHEPoint(point, OHEDict, numOHEFeats):\n    \"\"\"Obtain the label and feature vector for this raw observation.\n\n    Note:\n        You must use the function `oneHotEncoding` in this implementation or later portions\n        of this lab may not function as expected.\n\n    Args:\n        point (str): A comma separated string where the first value is the label and the rest\n            are features.\n        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n        numOHEFeats (int): The number of unique features in the training dataset.\n\n    Returns:\n        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n            raw features based on the provided OHE dictionary.\n    \"\"\"\n    return LabeledPoint(point.split(',')[0],oneHotEncoding(parsePoint(point), OHEDict, numCtrOHEFeats))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["OHETrainData = rawTrainData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\nOHETrainData.cache()\nprint OHETrainData.take(1)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["To explain the format of the resulting data points:"],"metadata":{}},{"cell_type":"code","source":["SparseVector(10, [(1,1), (6,1)])"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["LabeledPoint(0, SparseVector(10, [(1,1), (6,1)]))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["#### Handle unseen features\n\nCompute OHE features for the validation and test datasets. \n\n**Some categorical values will likely appear in new data that did not exist in the training data.**\n\nTo deal with this situation, update the oneHotEncoding() function to ignore previously unseen categories, and then compute OHE features for the validation data."],"metadata":{}},{"cell_type":"code","source":["def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n\n    Note:\n        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n        ignored.\n\n    Args:\n        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n            value).\n\n    Returns:\n        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n            identifiers for the (featureID, value) combinations that occur in the observation and\n            with values equal to 1.0.\n    \"\"\"\n    validFeatureTuples = []\n    for (featID, value) in rawFeats:\n        try:\n            validFeatureTuples.append((OHEDict[(featID, value)],1))\n        except KeyError:\n            pass\n    return SparseVector(numOHEFeats, validFeatureTuples)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["OHEValidationData = rawValidationData.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\nOHEValidationData.cache()\nprint OHEValidationData.take(1)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### 3. Model Construction\n\nChoose **logistic regression**. \n\nIt models the **probability of a click-through event** rather than returning a binary response, and when working with rare events, probabilistic predictions are useful."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.classification import LogisticRegressionWithSGD"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Set the **hyperparameters**."],"metadata":{}},{"cell_type":"code","source":["numIters = 50\nstepSize = 10.\nregParam = 1e-6\nregType = 'l2'\nincludeIntercept = True"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["**Train** the logistic regression model."],"metadata":{}},{"cell_type":"code","source":["model0 = LogisticRegressionWithSGD.train(OHETrainData,iterations=numIters,\n                                         step=stepSize,regParam=regParam,\n                                         regType=regType,intercept=includeIntercept)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["sortedWeights = sorted(model0.weights)\nprint sortedWeights[:5], model0.intercept"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### 4. Model Evaluation\n\nUse **log loss** to evaluate the quality of models."],"metadata":{}},{"cell_type":"markdown","source":["Write a function to compute **log loss**."],"metadata":{}},{"cell_type":"code","source":["from math import log\n\ndef computeLogLoss(p, y):\n    \"\"\"Calculates the value of log loss for a given probabilty and label.\n\n    Note:\n        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it\n        and when p is 1 we need to subtract a small value (epsilon) from it.\n\n    Args:\n        p (float): A probabilty between 0 and 1.\n        y (int): A label.  Takes on the values 0 and 1.\n\n    Returns:\n        float: The log loss value.\n    \"\"\"\n\n    epsilon = 10e-12\n    if p == 0:\n        p += epsilon\n    elif p == 1:\n        p -= epsilon\n    if y == 1:\n        return -log(p)\n    else:\n        return -log(1 - p)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["Use the log loss to evaluate some sample imputs."],"metadata":{}},{"cell_type":"code","source":["print computeLogLoss(.5, 1)\nprint computeLogLoss(.5, 0)\nprint computeLogLoss(.99, 1)\nprint computeLogLoss(.99, 0)\nprint computeLogLoss(.01, 1)\nprint computeLogLoss(.01, 0)\nprint computeLogLoss(0, 1)\nprint computeLogLoss(1, 1)\nprint computeLogLoss(1, 0)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["#### Baseline Log Loss\n\nAlways make the same prediction independent of the given datapoint, setting the predicted value equal to the fraction of training points that correspond to click-through events (i.e., where the label is one). \n\nCompute this value (which is simply the mean of the training labels), and then use it to compute the training log loss for the baseline model. The log loss for multiple observations is the mean of the individual log loss values."],"metadata":{}},{"cell_type":"code","source":["# Note that our dataset has a very high click-through rate by design\n# In practice click-through rate can be one to two orders of magnitude lower\nclassOneFracTrain = OHETrainData.map(lambda lp: lp.label).mean()\nprint classOneFracTrain\n\nlogLossTrBase = OHETrainData.map(lambda lp: computeLogLoss(classOneFracTrain, lp.label)).mean()\nprint 'Baseline Train Logloss = {0:.3f}\\n'.format(logLossTrBase)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["#### Predict the probability\n\nGenerate predictions from the model."],"metadata":{}},{"cell_type":"code","source":["from math import exp #  exp(-t) = e^-t\n\ndef getP(x, w, intercept):\n    \"\"\"Calculate the probability for an observation given a set of weights and intercept.\n\n    Note:\n        We'll bound our raw prediction between 20 and -20 for numerical purposes.\n\n    Args:\n        x (SparseVector): A vector with values of 1.0 for features that exist in this\n            observation and 0.0 otherwise.\n        w (DenseVector): A vector of weights (betas) for the model.\n        intercept (float): The model's intercept.\n\n    Returns:\n        float: A probability between 0 and 1.\n    \"\"\"\n    rawPrediction = w.dot(x) + intercept\n\n    # Bound the raw prediction value\n    rawPrediction = min(rawPrediction, 20)\n    rawPrediction = max(rawPrediction, -20)\n    return 1/(1 + exp(-rawPrediction))"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["trainingPredictions = OHETrainData.map(lambda lp: getP(lp.features, model0.weights, model0.intercept))\n\nprint trainingPredictions.take(5)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["#### Evaluate the Model"],"metadata":{}},{"cell_type":"code","source":["def evaluateResults(model, data):\n    \"\"\"Calculates the log loss for the data given the model.\n\n    Args:\n        model (LogisticRegressionModel): A trained logistic regression model.\n        data (RDD of LabeledPoint): Labels and features for each observation.\n\n    Returns:\n        float: Log loss for the data.\n    \"\"\"\n    return (data\n            # First get the predictions.\n            .map(lambda lp: (lp.label, getP(lp.features, model.weights, model.intercept)))\n            # Then use the predictions to compute the log loss\n            .map(lambda (label, prediction): computeLogLoss(prediction, label))\n            .mean())"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["logLossTrLR0 = evaluateResults(model0, OHETrainData)\nprint ('OHE Features Train Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(logLossTrBase, logLossTrLR0))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["#### Validation Log Loss\n\nCompute the validation log loss for both the baseline and logistic regression models."],"metadata":{}},{"cell_type":"code","source":["logLossValBase = OHEValidationData.map(lambda lp: computeLogLoss(classOneFracTrain, lp.label)).mean()\n\nlogLossValLR0 = evaluateResults(model0, OHEValidationData)\nprint ('OHE Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(logLossValBase, logLossValLR0))"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["#### ROC Curve\n\nShow the trade-off between the false positive rate and true positive rate."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\nx, y = zip(*featCountsBuckets)\nx, y = np.log(x), np.log(y)\n\ndef preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n                gridWidth=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["labelsAndScores = OHEValidationData.map(lambda lp:\n                                            (lp.label, getP(lp.features, model0.weights, model0.intercept)))\nlabelsAndWeights = labelsAndScores.collect()\nlabelsAndWeights.sort(key=lambda (k, v): v, reverse=True)\nlabelsByWeight = np.array([k for (k, v) in labelsAndWeights])\n\nlength = labelsByWeight.size\ntruePositives = labelsByWeight.cumsum()\nnumPositive = truePositives[-1]\nfalsePositives = np.arange(1.0, length + 1, 1.) - truePositives\n\ntruePositiveRate = truePositives / numPositive\nfalsePositiveRate = falsePositives / (length - numPositive)\n\n# Generate layout and plot data\nfig, ax = preparePlot(np.arange(0., 1.1, 0.1), np.arange(0., 1.1, 0.1))\nax.set_xlim(-.05, 1.05), ax.set_ylim(-.05, 1.05)\nax.set_ylabel('True Positive Rate (Sensitivity)')\nax.set_xlabel('False Positive Rate (1 - Specificity)')\nplt.plot(falsePositiveRate, truePositiveRate, color='#8cbfd0', linestyle='-', linewidth=3.)\nplt.plot((0., 1.), (0., 1.), linestyle='--', color='#d6ebf2', linewidth=2.)  # Baseline model\npass"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["display(fig)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["### 5. Reduce Feature Dimension"],"metadata":{}},{"cell_type":"code","source":["from collections import defaultdict\nimport hashlib\n\ndef hashFunction(numBuckets, rawFeats, printMapping=False):\n    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n\n    Note:\n        Use printMapping=True for debug purposes and to better understand how the hashing works.\n\n    Args:\n        numBuckets (int): Number of buckets to use as features.\n        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n            (featureID, value) tuples.\n        printMapping (bool, optional): If true, the mappings of featureString to index will be\n            printed.\n\n    Returns:\n        dict of int to float:  The keys will be integers which represent the buckets that the\n            features have been hashed to.  The value for a given key will contain the count of the\n            (featureID, value) tuples that have hashed to that key.\n    \"\"\"\n    mapping = {}\n    for ind, category in rawFeats:\n        featureString = category + str(ind)\n        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n    if(printMapping): print mapping\n    sparseFeatures = defaultdict(float)\n    for bucket in mapping.values():\n        sparseFeatures[bucket] += 1.0\n    return dict(sparseFeatures)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["#### Create hashed features."],"metadata":{}},{"cell_type":"code","source":["def parseHashPoint(point, numBuckets):\n    \"\"\"Create a LabeledPoint for this observation using hashing.\n\n    Args:\n        point (str): A comma separated string where the first value is the label and the rest are\n            features.\n        numBuckets: The number of buckets to hash to.\n\n    Returns:\n        LabeledPoint: A LabeledPoint with a label (0.0 or 1.0) and a SparseVector of hashed\n            features.\n    \"\"\"\n    fields = point.split(',')\n    label = fields[0]\n    features = parsePoint(point)\n    return LabeledPoint(label, SparseVector(numBuckets, hashFunction(numBuckets, features)))"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["numBucketsCTR = 2 ** 15\nhashTrainData = rawTrainData.map(lambda point: parseHashPoint(point, numBucketsCTR))\nhashTrainData.cache()\nhashValidationData = rawValidationData.map(lambda point: parseHashPoint(point, numBucketsCTR))\nhashValidationData.cache()\nhashTestData = rawTestData.map(lambda point: parseHashPoint(point, numBucketsCTR))\nhashTestData.cache()\n\nprint hashTrainData.take(1)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["#### Re-train the model\n\nRun a **grid search** to find suitable hyperparameters for the hashed features, evaluating via log loss on the validation data."],"metadata":{}},{"cell_type":"code","source":["numIters = 500\nregType = 'l2'\nincludeIntercept = True\n\n# Initialize variables using values from initial model training\nbestModel = None\nbestLogLoss = 1e10"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["stepSizes = [1, 10]\nregParams = [1e-6, 1e-3]\nfor stepSize in stepSizes:\n    for regParam in regParams:\n        model = (LogisticRegressionWithSGD\n                 .train(hashTrainData, numIters, stepSize, regParam=regParam, regType=regType,\n                        intercept=includeIntercept))\n        logLossVa = evaluateResults(model, hashValidationData)\n        print ('\\tstepSize = {0:.1f}, regParam = {1:.0e}: logloss = {2:.3f}'\n               .format(stepSize, regParam, logLossVa))\n        if (logLossVa < bestLogLoss):\n            bestModel = model\n            bestLogLoss = logLossVa\n\nprint ('Hashed Features Validation Logloss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(logLossValBase, bestLogLoss))"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["#### Evaluate on the test set."],"metadata":{}},{"cell_type":"code","source":["# Log loss for the best model from (5d)\nlogLossTest = evaluateResults(bestModel, hashTestData)\n\n# Log loss for the baseline model\nlogLossTestBaseline = hashTestData.map(lambda lp: computeLogLoss(classOneFracTrain, lp.label)).mean()\n\nprint ('Hashed Features Test Log Loss:\\n\\tBaseline = {0:.3f}\\n\\tLogReg = {1:.3f}'\n       .format(logLossTestBaseline, logLossTest))"],"metadata":{},"outputs":[],"execution_count":73}],"metadata":{"name":"click_through_rate_prediction","notebookId":2004297364207044},"nbformat":4,"nbformat_minor":0}
